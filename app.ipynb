{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce451265",
   "metadata": {},
   "source": [
    "### 1. Environment Setup\n",
    "\n",
    "Before running the application, you need to set up your environment variables. This includes credentials for Modal, Hugging Face, and AWS. The following cells will help you create a `.env` file with the necessary credentials.\n",
    "\n",
    "**Important:** Replace the placeholder values (`xxx`) with your actual credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b710f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODAL_TOKEN_ID = \"xxx\"\n",
    "MODAL_TOKEN_SECRET = \"xxx\"\n",
    "MODAL_PROFILE = \"eikona-io\"\n",
    "HF_TOKEN = \"xxx\"\n",
    "\n",
    "AWS_ACCESS_KEY_ID = \"xxx\"\n",
    "AWS_SECRET_ACCESS_KEY = \"xxx\"\n",
    "AWS_DEFAULT_REGION = \"eu-north-1\"\n",
    "AWS_S3_BUCKET = \"eikona-io-ml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d689a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a .env file with these contents\n",
    "with open(\".env\", \"w\") as f:\n",
    "    f.write(f\"\"\"MODAL_TOKEN_ID={MODAL_TOKEN_ID}\n",
    "MODAL_TOKEN_SECRET={MODAL_TOKEN_SECRET}\n",
    "MODAL_PROFILE={MODAL_PROFILE}\n",
    "HF_TOKEN={HF_TOKEN}\n",
    "AWS_ACCESS_KEY_ID={AWS_ACCESS_KEY_ID}\n",
    "AWS_SECRET_ACCESS_KEY={AWS_SECRET_ACCESS_KEY}\n",
    "AWS_DEFAULT_REGION={AWS_DEFAULT_REGION}\n",
    "AWS_S3_BUCKET={AWS_S3_BUCKET}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b14038",
   "metadata": {},
   "source": [
    "### 2. Install Dependencies\n",
    "\n",
    "Next, install the required Python packages using the `requirements.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78addae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e75f8da",
   "metadata": {},
   "source": [
    "### 3. Run the Inference Server\n",
    "\n",
    "Now, open a new terminal and start the inference server using the Modal CLI. The command will deploy the `flux_serve.py` script to the Modal serverless platform.\n",
    "\n",
    "```sh\n",
    "# Command to start the server:\n",
    "modal serve flux_serve.py\n",
    "\n",
    "# Expected output:\n",
    "‚úì Initialized. View run at https://modal.com/apps/eikona-io/main/ap-hRzmyUVVgEHP6WwiqmuyCB\n",
    "‚úì Created objects.\n",
    "‚îú‚îÄ‚îÄ üî® Created mount /home/ubuntu/Modal-ML-Inference-Service/flux_serve.py\n",
    "‚îú‚îÄ‚îÄ üî® Created mount PythonPackage:schemas, PythonPackage:utils\n",
    "‚îú‚îÄ‚îÄ üî® Created web function fastapi_app => https://eikona-io--flux-fastapi-app-dev.modal.run\n",
    "‚îî‚îÄ‚îÄ üî® Created function FluxModel.*.\n",
    "Ô∏èÔ∏è‚ö°Ô∏è Serving... hit Ctrl-C to stop!\n",
    "‚îú‚îÄ‚îÄ Watching /home/ubuntu/Modal-ML-Inference-Service/schemas.\n",
    "‚îú‚îÄ‚îÄ Watching /home/ubuntu/Modal-ML-Inference-Service/utils.\n",
    "‚îî‚îÄ‚îÄ Watching /home/Modal-ML-Inference-Service.\n",
    "‚†º Running app...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18bccf4",
   "metadata": {},
   "source": [
    "### 4. Prepare the Inference Request\n",
    "\n",
    "Now that the server is running, you can prepare the request payload. The following cell defines the parameters for the image generation request, such as the prompt, number of images, and diversity settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c05109b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"a photo of a corgi surfing a wave at sunset\"\n",
    "GPUS = 2\n",
    "NOF_IMAGES = 4\n",
    "SAMPLERS = [\"euler\", \"ddim\", \"dpmpp_2m\"] # Only euler supported\n",
    "NOF_STEPS = [10, 20, 30]\n",
    "CFG_SCALES = [3.5, 5.0, 7.5]\n",
    "SEEDS = 'mix' # Feature not implemented yet\n",
    "STREAM = False # Feature not implemented yet\n",
    "TOP_K = None # Feature not implemented yet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91f30ea",
   "metadata": {},
   "source": [
    "### 5. Send the Request and Get the Results\n",
    "\n",
    "Finally, send the request to the inference server and print the response. The response will contain the URLs of the generated images and other metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff5e2d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'a photo of a corgi surfing a wave at sunset',\n",
       " 'count': 4,\n",
       " 'images': [{'url': 'https://eikona-io-ml.s3.eu-north-1.amazonaws.com/images/flux-1756310122703.jpg',\n",
       "   'sampling_params': {'sampler': 'EulerDiscreteScheduler',\n",
       "    'steps': 30,\n",
       "    'cfg_scale': 5.0,\n",
       "    'seed': 422596259}},\n",
       "  {'url': 'https://eikona-io-ml.s3.eu-north-1.amazonaws.com/images/flux-1756310126768.jpg',\n",
       "   'sampling_params': {'sampler': 'EulerDiscreteScheduler',\n",
       "    'steps': 10,\n",
       "    'cfg_scale': 5.0,\n",
       "    'seed': 771367970}},\n",
       "  {'url': 'https://eikona-io-ml.s3.eu-north-1.amazonaws.com/images/flux-1756310122746.jpg',\n",
       "   'sampling_params': {'sampler': 'EulerDiscreteScheduler',\n",
       "    'steps': 30,\n",
       "    'cfg_scale': 3.5,\n",
       "    'seed': 1960236773}},\n",
       "  {'url': 'https://eikona-io-ml.s3.eu-north-1.amazonaws.com/images/flux-1756310126713.jpg',\n",
       "   'sampling_params': {'sampler': 'EulerDiscreteScheduler',\n",
       "    'steps': 20,\n",
       "    'cfg_scale': 3.5,\n",
       "    'seed': 1225293127}}],\n",
       " 'latency_ms': 8034}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://eikona-io--flux-fastapi-app-dev.modal.run/generate\"\n",
    "\n",
    "payload = {\n",
    "    \"prompt\": PROMPT,\n",
    "    \"nof_images\": NOF_IMAGES,\n",
    "    \"gpus\": GPUS,\n",
    "    \"diversity\": {\n",
    "        \"samplers\": SAMPLERS,\n",
    "        \"nof_steps\": NOF_STEPS,\n",
    "        \"cfg_scales\": CFG_SCALES,\n",
    "        \"seeds\": SEEDS\n",
    "    },\n",
    "    \"stream\": STREAM,\n",
    "    \"top_k\": TOP_K\n",
    "}\n",
    "\n",
    "resp = requests.post(url, json=payload, timeout=180)\n",
    "resp.raise_for_status()  # raises if non-2xx\n",
    "resp.json()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
